# -*- coding: utf-8 -*-
"""OCR_CNN_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wGvwoWiolaleefZ3QbpRhlpgE4FD6juu
"""

import numpy as np
from PIL import Image
import cv2
from matplotlib import pyplot as plt
import numpy as np
# import imutils
# import pytesseract
import mysql.connector
from mysql.connector import Error
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models
# from tensorflow.keras.datasets import mnist
from sklearn.model_selection import train_test_split
# from skimage import io, color, morphology
import os
import cv2
import pandas as pd
from sklearn.model_selection import train_test_split

# Define paths
data_dir = 'E:/Number Plate Prediction/Dataset/data/training_data'

# Initialize lists to hold image data and labels
images_data = []
labels = []
char_to_num = {chr(i): i - 65 for i in range(65, 91)}  # a-z to 0-25
char_to_num.update({str(i): i + 26 for i in range(10)})  # 0-9 to 26-35
# Iterate over each folder (0-9, a-z)
for folder in os.listdir(data_dir):
    folder_path = os.path.join(data_dir, folder)
    if os.path.isdir(folder_path):
        for img_file in os.listdir(folder_path):
            # Read the image in grayscale
            img_path = os.path.join(folder_path, img_file)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
            img = cv2.resize(img, (28, 28))  # Resize to match the input shape of the CNN model
            images_data.append(img)
            labels.append(char_to_num[folder])
images_data = np.array(images_data, dtype='float32')
images_data = images_data / 255.0
images_data = images_data.reshape((images_data.shape[0], 28, 28, 1))

labels = np.array(labels, dtype="int")
# Create a dataframe with two columns: 'image' and 'label'
(azData, azLabels)=(images_data,labels)

x_train, x_test, y_train, y_test = train_test_split(azData, azLabels, test_size=0.2, random_state=42)

plt.imshow(img.squeeze(),cmap='gray')  # Remove single-dimensional entries from the shape

from tensorflow.keras import layers, models

model_alpha = models.Sequential([
    # First Convolutional Layer
    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),

    # Second Convolutional Layer
    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),

    # Third Convolutional Layer
    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),

    # Fourth Convolutional Layer
    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),

    # Fifth Convolutional Layer
    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),

    # Global Average Pooling
    layers.GlobalAveragePooling2D(),

    # Dense Layers
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(36, activation='softmax')  # For MNIST digits
])

model_alpha.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

# Train the model
history = model_alpha.fit(x_train, y_train, epochs=7,
                        validation_data=(x_test, y_test))

# Evaluate the model
test_loss, test_acc = model_alpha.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

model_alpha.save('E:/Number Plate Prediction/model/ocr.h5')

def straighten_image(image):
    # Convert the image to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply edge detection
    edges = cv2.Canny(gray, 50, 150, apertureSize=3)

    # Apply Hough Line Transform to detect lines in the image
    lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)

    # Calculate the angle of the skew
    if lines is not None:
        angles = []
        for line in lines:
            rho, theta = line[0]
            angle = np.degrees(theta)
            if angle > 45:
                angle -= 90
            angles.append(angle)

        # Find the median angle of the lines detected
        skew_angle = np.median(angles)
    else:
        skew_angle = 0  # No lines detected, assume no skew

    # Get the dimensions of the image
    (h, w) = image.shape[:2]

    # Calculate the center of the image
    center = (w // 2, h // 2)

    # Calculate the rotation matrix to correct the skew
    M = cv2.getRotationMatrix2D(center, skew_angle, 1.0)

    # Rotate the image to straighten it
    straightened = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)

    return straightened

import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model

def extract_and_segment_characters(image):
    # Convert to grayscale
#     image=cv2.resize(image, (100, 100))
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply Gaussian blurring
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)

    # Apply edge detection
    edges = cv2.Canny(blurred, 50, 150)

    # Find contours
    contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    # Find the contour with the largest area
    contour = max(contours, key=cv2.contourArea)

    # Get the bounding rectangle for the largest contour
    x, y, w, h = cv2.boundingRect(contour)

    # Extract the number plate region
    number_plate = image[y:y+h, x:x+w]
    number_plate = straighten_image(number_plate)
    number_plate = cv2.resize(number_plate, (900, 300))

    # Convert the number plate region to grayscale
    number_plate_gray = cv2.cvtColor(number_plate, cv2.COLOR_BGR2GRAY)

    # Apply adaptive thresholding to the number plate region
    thresh = cv2.adaptiveThreshold(number_plate_gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 45, 15)

    # Find contours in the thresholded number plate region
    char_contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Filter contours based on size and aspect ratio
    filtered_contours = []
    for contour in char_contours:
        x, y, w, h = cv2.boundingRect(contour)
        aspect_ratio = w / float(h)
        if 0.2 < aspect_ratio < 1.0 and 1000 < cv2.contourArea(contour):
            filtered_contours.append((x,contour))
            cv2.rectangle(number_plate, (x, y), (x + w, y + h), (0, 255, 0), 2)
    filtered_contours.sort(key=lambda k: k[0])
    return number_plate, filtered_contours

def preprocess_char_img(char_img):
    if char_img.shape[-1] == 3:  # Check if the image has 3 channels (RGB)
        char_img = cv2.cvtColor(char_img, cv2.COLOR_BGR2GRAY)
    char_img = cv2.resize(char_img, (28, 28))  # Resize to match the input shape of the CNN model
    char_img = char_img / 255.0  # Normalize to [0, 1]
    char_img = np.expand_dims(char_img, axis=-1)  # Add channel dimension (28, 28, 1)
    char_img = np.expand_dims(char_img, axis=0)  # Add batch dimension (1, 28, 28, 1)
    return char_img

def predict(file_name):
    model_alpha=tf.keras.models.load_model('E:/Number Plate Prediction/model/ocr.h5')
    image = cv2.imread(file_name)

    # Extract and segment characters
    number_plate, fc = extract_and_segment_characters(image)

    # Display the extracted number plate
    plt.imshow(cv2.cvtColor(number_plate, cv2.COLOR_BGR2RGB))
    plt.title('Extracted Number Plate')
    plt.axis('off')
    plt.show()


    recognized_text = ''
    recognized_text1 = ''

    char_contours=[]
    for i in  fc:
        char_contours.append(i[1])
    for i, contour in enumerate(char_contours):
        x, y, w, h = cv2.boundingRect(contour)
        char_img = number_plate[y-10:y+h+20, x-5:x+w+5]
        char_img = straighten_image(char_img)
        char_img = cv2.cvtColor(char_img, cv2.COLOR_BGR2GRAY)
    #     char_img = cv2.bitwise_not(char_img)
        _, char_img = cv2.threshold(char_img, 145, 255, cv2.THRESH_BINARY)
    #     char_img = morphology.skeletonize(char_img)
    #     char_img = (char_img * 255).astype(np.uint8)
    #     kernel = np.ones((3, 3), np.uint8)
    #     char_img = cv2.dilate(char_img, kernel, iterations=3)

        char_img = preprocess_char_img(char_img)  # Preprocess the character image
        # Predict the character using the CNN model
        prediction = model_alpha.predict(char_img)
        print(prediction[0][0:26],i,len(char_contours))
        f=0
        if ((0<=i<=1 or 4<=i<=5) and len(char_contours)==10) or ((0<=i<=1 or 3<=i<=4) and len(char_contours)==9):
            print('here')
            recognized_char_index = np.argmax(prediction[0][0:26])  # Get the index of the highest probability
        else:
            print('there')
            recognized_char_index = np.argmax(prediction[0][26:])
            f=1
    #     prediction1 = model_alpha.predict(char_img)
    #     recognized_char_index_1 = np.argmax(prediction1)
        # Assuming the model output indices correspond to characters 0-9 and A-Z
        if f==1:
            recognized_char = str(recognized_char_index)  # For digits 0-9
        else:
            recognized_char = chr(65 + recognized_char_index)  # For letters A-Z

        recognized_text += recognized_char if recognized_char else '?'
        #recognized_text1 += recognized_char1 if recognized_char1 else '?'

        # Display the character and its predicted label
        plt.imshow(char_img.squeeze(),cmap='gray')  # Remove single-dimensional entries from the shape
        plt.title(f'Numb {i+1}: {recognized_char_index} ')
        plt.axis('off')
        plt.show()

    print("Recognized Text:", recognized_text)
    print("Recognized Text:", recognized_text1)

predict('E:/Number Plate Prediction/R.jpg')

